<!DOCTYPE html>
<html>

<head>

  <!-- meta -->
  <meta charset="utf-8" content="width=device-width, initial-scale=1.0" name="viewport" />
  <link href="./style.css" rel="stylesheet" type="text/css" />

  <!-- mathjax -->
  <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML"></script>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [
          ['$', '$']
        ],
        displayMath: [
          ['$$', '$$']
        ],
        processEscapes: true,
        processEnvironments: true,
        packages: ['base', 'ams']
      },
      svg: {
        fontCache: 'global'
      }
    });
  </script>

  <!-- code highlight -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.7.2/styles/default.min.css" rel="stylesheet"/>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.7.2/highlight.min.js"></script>
  <script> hljs.highlightAll(); </script>

  <style>
    .strike {
      position: relative;
    }

    .strike::before {
      content: "";
      position: absolute;
      top: 0;
      left: 0;
      width: 100%;
      height: 100%;
      background: linear-gradient(to bottom right, transparent calc(50% - 1px), #cd3700, transparent calc(50% + 1px));
    }
  </style>
 
  <title> DPO </title>

</head>

<body> 
  <div class="container">
    <h2 class="title"> Direct Preference Optimization </h2>
    
    <h3><a id="Bradley-Terry-model" href="#Bradley-Terry-model" class="heading">Bradley-Terry model</a></h3>

    <p>
      Suppose a bunch of people have various preferences for something, say, person A rates a cookie 5/10, person B rates it 7/10, and so on. Or suppose that we have a bunch of players in a game and each gained a particular score. What's the probability that a particular will be rated higher than another by two people? Or what's the probability that one player will win against another? We can model this is as follows 
    </p>

    <p class="display-math">
      $$p(y_1 \succ y_2) = \frac{e^{\beta_1}}{e^{\beta_1} + e^{\beta_2}}$$
    </p>

    <p>
      where $\beta_1$ and $\beta_2$ are the scores assigned to a particular thing by two score assigners. In the cookie case, we have $p(A \succ B)$, i.e., the probability that the cookie will be preferred by A over B, and $\beta_1$ and $\beta_2$ are the scores assigned by A and B respectively. The denominator normalizes, i.e., ensures that the probabilities sum to 1.
    </p>

    <h3> <a id="rlhf" href="#rlhf" class="heading"> RLHF objective </a></h3>

    <p>
      We let LLMs generate completions $y_1$ and $y_2$ for a prompt $x$ and human annotators rank the completions, i.e., they state their preferences for the completions. Let's denote the ranking mechanism used by the annotators as $r^*(x, y_i)$ where $i \in \{1, 2\}$ and $r^*$ is the metric by which the annotators assign scores to a completion $y_i$ for a prompt $x$. Then as per the BT model, we have 
    </p>

    <p class="display-math">
      \begin{align*}
        p(y_i \succ y_j) &= \frac{e^{r^*(x,\,y_i)}}{e^{r^*(x,\,y_i)} + e^{r^*(x,\,y_j)}} \\
        &= \frac{1}{1 + e^{(r^*(x,\,y_j) - r^*(x,\,y_i))}} \tag{1}
      \end{align*}
    </p>

    <p>
      where we have divide the numerator and denominator by $r^*(x, y_i)$. $r^*$ is what we first aim to learn in classic RLHF, and we do this by training a reward model $r_{\phi}$ parametrized by $\phi$. We then use this reward model to optimize the LLM using an off the shelf online RL algorithm, such as PPO. The objective for the RL phase is
    </p>

    <p class="display-math">
      $$\max_{\pi_{\theta}}\,\mathbb{E}_{x\,\sim\,\mathcal{D},\,y\,\sim\,\pi_{\theta}(y\,|\,x)}\left[r_{\phi}(x, y)\right] - \beta D_{KL}(\pi_{\theta}(y\,|\,x)\,||\,\pi_{\text{ref}}(y\,|\,x)) \tag{2}$$
    </p>

    <p>
      where the second term is the KL divergence term that tries to prevent our model $\pi_{\theta}$ from deviating too much from the reference pre-trained model $\pi_{\text{ref}}$ and $\beta$ is the strength of the penalty. Now, RLHF is expensive because it's a two-step process: first training a reward model, then generating trajectories using the model to be trained. Using a reward <i>model</i> instead of the true reward also feels janky. The reward model is not perfect, and the LM has to learn to adapt to the imperfect reward model - you're compounding errors.  
    </p>

    <p>
      Is there a way to frame the objective such that it only depends on the output of the policy model $\pi_{\theta}$ and the reference model $\pi_{\text{ref}}$ and not the learned reward model $r_{\phi}$, effectively eliminating the reward model training step? Enter DPO.
    </p>

    <h3><a id="reframing" href="#reframing" class="heading">Reframing the RLHF objective</a></h3>

    <p>
      We can rewrite the reward objective in $(2)$ like so
    </p>

    <p class="display-math">
      \begin{align*}
        & \max_{\pi_{\theta}} \mathbb{E}_{x\,\sim\,\mathcal{D},\,y\,\sim\,\pi_{\theta}(y\,|\,x)}\left[r(x, y)\right] - \beta D_{KL}(\pi_{\theta}(y\,|\,x)\,||\,\pi_{\text{ref}}(y\,|\,x)) \\
        &= \max_{\pi_{\theta}} \mathbb{E}_{x\,\sim\,\mathcal{D},\,y\,\sim\,\pi_{\theta}(y\,|\,x)}\left[r(x, y) - \beta \log\frac{\pi_{\theta}(y\,|\,x)}{\pi_{\text{ref}}(y\,|\,x)}\right] \\
        &= \min_{\pi_{\theta}} \mathbb{E}_{x\,\sim\,\mathcal{D},\,y\,\sim\,\pi_{\theta}(y\,|\,x)}\left[\beta \log\frac{\pi_{\theta}(y\,|\,x)}{\pi_{\text{ref}}(y\,|\,x)} - r(x, y)\right] \\
        &= \min_{\pi_{\theta}} \mathbb{E}_{x\,\sim\,\mathcal{D},\,y\,\sim\,\pi_{\theta}(y\,|\,x)}\left[\log\frac{\pi_{\theta}(y\,|\,x)}{\pi_{\text{ref}}(y\,|\,x)} - \frac{1}{\beta}r(x, y)\right] \\
        &= \min_{\pi_{\theta}} \mathbb{E}_{x\,\sim\,\mathcal{D},\,y\,\sim\,\pi_{\theta}(y\,|\,x)}\left[\log\frac{\pi_{\theta}(y\,|\,x)}{\pi_{\text{ref}}(y\,|\,x)} - \log\exp\left(\frac{1}{\beta}r(x, y)\right)\right] \\
        &= \min_{\pi_{\theta}} \mathbb{E}_{x\,\sim\,\mathcal{D},\,y\,\sim\,\pi_{\theta}(y\,|\,x)}\left[\log\frac{\pi_{\theta}(y\,|\,x)}{\frac{1}{Z(x)}\pi_{\text{ref}}(y\,|\,x)\exp\left(\frac{1}{\beta}r(x, y)\right)} - \log Z(x)\right] 
      \end{align*}
    </p>

    <p>
      where
    </p>

    <p class="display-math">
      $$Z(x) = \sum_y \pi_{\text{ref}}(y\,|\,x)\exp\left(\frac{1}{\beta}r(x, y)\right)$$
    </p>

    <p>
      and is the normalizing constant. We can see that the optimally trained model $\pi_{\theta}^*$ is given by
    </p>

    <p class="display-math">
      $$\pi_{\theta}^*(y\,|\,x) = \frac{1}{Z(x)}\pi_{\text{ref}}(y\,|\,x)\exp\left(\frac{1}{\beta}r(x, y)\right) \tag{3}$$
    </p>

    <p>
      because this minimizes the KL divergence where $\pi_{\text{ref}}$ is our reference model (i.e., the pre-trained model) and $r$ is the reward metric denoting the human annotation process. We need to get rid of $r$. If $\pi^*_{\theta}$ is the optimal policy model, then the ground truth reward $r^*$ is given by
    </p>

    <p class="display-math">
      $$r^*(x, y) = \beta \log\frac{\pi_{\theta}^*(y\,|\,x)}{\pi_{\text{ref}}(y\,|\,x)} + \beta \log Z(x) \tag{4}$$
    </p>

    <p>
      which you can get with a bit of algebra on $(3)$. What this is saying is this: suppose that we have an RL tuned model $\pi_{\theta}^*$ that perfectly satisfies the human annotators' preference function $r^*$ ($r^*$ is basically the heuristic or underlying reward model that represents the human annotators and this is what we learn in classic RLHF with a reward model $r_{\phi}$). Then $r^*$ is given by $(4)$. This is the score given for a completion $y$ given a prompt $x$. We can substitute this score in the Bradley-Terry preference model to get the probability that completion $y_1$ is preffered over completion $y_2$ given prompt $x$
    </p>

    <p class="display-math">
      \begin{align*}
        p^*(y_1 \succ y_2\,|\,x) &= \frac{1}{1 + \exp\left(\beta\log\frac{\pi_{\theta}^*(y_2\,|\,x)}{\pi_{\text{ref}}(y_2\,|\,x)} - \beta\log\frac{\pi_{\theta}^*(y_1\,|\,x)}{\pi_{\text{ref}}(y_1\,|\,x)}\right)} \\
        &= \sigma\left(\beta\log\frac{\pi_{\theta}^*(y_1\,|\,x)}{\pi_{\text{ref}}(y_1\,|\,x)} - \beta\log\frac{\pi_{\theta}^*(y_2\,|\,x)}{\pi_{\text{ref}}(y_2\,|\,x)}\right) \tag{5}
      \end{align*}
    </p>

    <p>
      where $\sigma$ is the sigmoid function. It's conventient that the $Z(x)$s cancel out, because it depends on $r(x, y)$ and we're trying to get rid of it from the objective. If $y_w$ denotes the favorable completion and $y_l$ denotes the completion that sucks (as rated by the human annotator), substituting $y_w$ and $y_l$ for $y_1$ and $y_2$ in $(5)$ gives
    </p>

    <p class="display-math">
      \begin{align*}
        p^*(y_w \succ y_l\,|\,x) = \sigma\left(\beta\log\frac{\pi_{\theta}^*(y_w\,|\,x)}{\pi_{\text{ref}}(y_w\,|\,x)} - \beta\log\frac{\pi_{\theta}^*(y_l\,|\,x)}{\pi_{\text{ref}}(y_l\,|\,x)}\right) \tag{6}
      \end{align*}
    </p>

    <p>
      and the above probability is maximal for the optimal policy $\pi_{\theta}^*$, i.e., we have 
    </p>
    
    <p class="display-math">
      $$p^*(y_w \succ y_l\,|\,x) \geq p^*(y_l \succ y_w\,|\,x)$$
    </p>

    <p>
      Equation $(6)$ is for the optimal model, and our goal then is to maximize
    </p>

    <p class="display-math">
      \begin{align*}
        p(y_w \succ y_l\,|\,x) = \sigma\left(\beta\log\frac{\pi_{\theta}(y_w\,|\,x)}{\pi_{\text{ref}}(y_w\,|\,x)} - \beta\log\frac{\pi_{\theta}(y_l\,|\,x)}{\pi_{\text{ref}}(y_l\,|\,x)}\right) \tag{6}
      \end{align*}
    </p>

    <p>
      i.e., we maximize the log-likelihood of the preference $y_w \succ y_l$ for a dataset $\mathcal{D}$ with prompts $x$, preffered completions $y_w$, and bad completions $y_l$. Our objective becomes minimizing the <i>negative</i> log-likelihood
    </p>

    <p class="display-math">
      $$\mathcal{L}_{\text{DPO}}(\pi_{\theta}\,;\,\pi_{\text{ref}}) = -\mathbb{E}_{(x,\,y_w,\,y_l)\,\sim\,\mathcal{D}}\left[\log \sigma\left(\beta\log\frac{\pi_{\theta}(y_w\,|\,x)}{\pi_{\text{ref}}(y_w\,|\,x)} - \beta\log\frac{\pi_{\theta}(y_l\,|\,x)}{\pi_{\text{ref}}(y_l\,|\,x)}\right)\right]$$
    </p>

    <p>
      and as desired, we have removed the reward model out of the picture :D
    </p>
  </div>
</body>
</html>
