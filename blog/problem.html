<!DOCTYPE html>
<html>

<head>

  <!-- meta -->
  <meta charset="utf-8" content="width=device-width, initial-scale=1.0" name="viewport" />
  <link href="../style.css" rel="stylesheet" type="text/css" />

  <!-- mathjax -->
  <script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML"></script>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [
          ['$', '$'],
        ],
        displayMath: [
          ['$$', '$$'],
        ],
        processEscapes: true,
        processEnvironments: true,
        packages: ['base', 'ams']
      },
      svg: {
        fontCache: 'global'
      }
    });
  </script>

  <!-- code highlight -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.7.2/styles/default.min.css"
    rel="stylesheet" />
  <script
    src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.7.2/highlight.min.js"></script>
  <script> hljs.highlightAll(); </script>

  <title> Just a Token Predictor! </title>

</head>

<body>

  <div class="container">
    <a id="back" href="../index.html">‚Üê</a>

    <h2 class="title"> "Just a Token Predictor!" </h2>

    <h3> <a href="#compression" id="compression" class="heading"> Compression and intelligence </a> </h3>

    <p>
      A popular argument against the "They're just next token predictors, so they don't understand anything!" critique of language models is that text prediction is compression, and compression leads to understanding. We turn to Ilya Sutskever's rather prescient quote from a <a href="https://icml.cc/2011/papers/524_icmlpaper.pdf">2011 paper</a> on RNNs:
    </p>

    <p>
      <i>More speculatively, achieving the asymptotic limit in text compression requires an understanding that is "equivalent to intelligence" (Hutter, 2006).</i>
    </p>

    <p>
      
    </p>
  </div>

</body>

</html>