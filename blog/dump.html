<!DOCTYPE html>
<html>

<head>

    <!-- meta -->
    <meta charset="utf-8" content="width=device-width, initial-scale=1.0" name="viewport" />
    <link href="../style.css" rel="stylesheet" type="text/css" />

    <!-- mathjax -->
    <script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML"></script>
    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [
          ['$', '$']
        ],
        displayMath: [
          ['$$', '$$']
        ],
        processEscapes: true,
        processEnvironments: true,
        packages: ['base', 'ams']
      },
      svg: {
        fontCache: 'global'
      }
    });
  </script>

    <!-- code highlight -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.7.2/styles/default.min.css"
        rel="stylesheet" />
    <script
        src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.7.2/highlight.min.js"></script>
    <script> hljs.highlightAll(); </script>

    <title> GPT-ception </title>

</head>

<body>
    <div class="container">
        <a id="back" href="../index.html">←</a>

        <h2 class="title"> Yann Lecun tweet explained (?) </h2>

        <div align="center">
            <blockquote class="twitter-tweet">
                <p lang="en" dir="ltr">* Language is low bandwidth: less than 12 bytes/second. A
                    person can read 270 words/minutes, or 4.5 words/second, which is 12 bytes/s
                    (assuming 2 bytes per token and 0.75 words per token). A modern LLM is typically
                    trained with 1x10^13 two-byte tokens, which is 2x10^13 bytes.… <a
                        href="https://t.co/FtCnxkVukK">https://t.co/FtCnxkVukK</a></p>&mdash; Yann
                LeCun (@ylecun) <a
                    href="https://twitter.com/ylecun/status/1766498677751787723?ref_src=twsrc%5Etfw">March
                    9, 2024</a>
            </blockquote>
            <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
        </div>

        <p>
            Now, Yann has thrown a lot of words here, but let's view it from the lens of <a
                href="https://en.wikipedia.org/wiki/Moravec's_paradox">Moravec's paradox</a> because
            it's largely similar (in my opinion).
        </p>

        <p>
            What does Yann mean by "...redundancy in data is *precisely* what we need for
            Self-Supervised Learning to capture the structure of the data"? So far, I have tried
            coming up with two similar explanations for two people who asked few days ago so here's
            the gist.
        </p>

        <p>
            (Below is messages formatted by Claude, removing some errors, so might have some issues)
        </p>

        <p>A 4-year-old can play 20 questions spontaneously. They are "embedded agents" -
            intuitively understanding the relation between them, their thoughts, and the world. They
            can answer "where and when did you learn this?" - the questions that are of interest for
            autonomous, agentic systems aren't the questions GPT-4 can easily answer. Answering
            graduate-level math questions is in no way going to help you with, say, something that
            can help you run a factory or engage with you daily in the physical world (where you
            don't have a laptop in front of you) over long time horizons. 4-year-olds have an
            intuitive physics model; a lot of knowledge is tacit and can't be said in words.</p>

        <p>Any question to do with episodic memory - you could say that by increasing the context
            length, ChatGPT can remember old conversations. But I'm not sure you can fit an entire
            life in ChatGPT's context length because of a lack of grounded reasoning: tacit
            knowledge of the current state of the world. Note that ChatGPT can only tell you the
            current date if it's in the system prompt. The date is a real fact about the current
            state of the world and is just a few bytes long snippet of information. The set of all
            information about the current state of the world, including those very contextually
            specific about the agent, however, can't be specified using just a few bytes of words
            (such as temperature, mood, weather, human interactions, the things you see as you
            interact - basically short term, episodic, and long term history of data ingested by
            your senses and internal climate like hormones and thoughts conditioning your current
            behavior) – you can't fit this in a simple "system prompt" because you cannot state them
            in words. Think of how incredibly sensitive your fingers are, the resolution of nerve
            input. When we say "ChatGPT is not grounded," it doesn't mean it's stupid, it means it
            doesn't have access to all the information required to answer a particular question,
            because getting that information is an engineering problem that can only be solved by
            building robots that get real-time information from the world across modalities. This is
            what Moravec's paradox is about. Text is way too compressed to get all that information,
            and so any question in this category is not answerable by an LLM.</p>

        <p>The difference between AGI (things that replace human labor) and GPT-4 (something that
            can answer complex questions) is tacit knowledge about the world. A simple example:
            interacting with ChatGPT voice. When you ask it to "read the code aloud," it will keep
            telling you to open the app and look at the markdown because that's how they programmed
            it - "if markdown generated, ask user to open the app and read the code." It doesn't
            understand the interface with which it interacts with the world. You need to
            deliberately talk to it and explain, "When you generate markdown, the TTS logic tells
            the user to look at the app. Instead, please don't write the code in markdown but read
            it word by word by generating a paragraph with all the words separated by a space.
            Replace the symbols with their name in the para, 'bracket', 'comma', and so on." These
            are the kinds of questions it can't answer. A 4-year-old intuitively understands how
            they're interfaced with the world. You don't notice that they're able to do this because
            it's so embedded in the way animals and humans behave - we find it natural. ChatGPT
            doesn't. This is why if tomorrow it can suddenly get a gold in all the Olympiads, the
            world won't change at all, because you still haven't solved the engineering problem of
            interfacing the system with real-time information about the world so it can intuitively
            model and interact with it. In short, instead of measuring AGI by asking "what sort of
            questions it can answer," we should measure by "what can it do." Measure by ability to
            do labor, not exam scores and academic benchmarks.</p>

        <p>
            Coming to the Figure demo: the demo does ground the LLM in the real world by interfacing
            it properly, but like the ChatGPT TTS example above, you'll be able to find very blatant
            issues of the same sort after interacting with it for some time. To get at where it
            might fail, you don't think of simply hard reasoning problems but about how the system
            design is set up - how are the various models integrated with one another and where does
            information about grounding fail to pass? Finding these failure mode questions will get
            harder and harder as more people do the sort of stuff like Figure does, but in my
            limited experience I'd still say that a lot of human labor requires bypassing these
            failure modes <i>very</i> consistently over a long time horizon.
        </p>

        <p>The issue is also how people interpret Yann. When Yann says "intelligent," he doesn't
            mean "something that can answer a large range of questions" - he also includes
            grounding, self-awareness, etc., in his definition. The latter is actually what matters
            for generality, because when you talk about "ability to do labor," you include the whole
            range of human activity, whereas if by intelligence you mean ability to answer
            questions, then you include only, say, white-collar work like math research or code
            assistance. If you only give a minute, a young teen may lose to an LLM in generating
            some bit of code. However, if you consider a span of a month or two, you can expect a
            teen to self-learn and make a reasonable piece of software, whereas without manually
            teaching the model to plan and an agent API, you can't expect it to automatically do
            anything. Even if ChatGPT has more knowledge, you're still going to trust a teenager
            with "Hey, I'll give you a month, make this software for me." LLM interactions last at
            most a few minutes, so Yann seems perfectly in the right to me. The slow nature of human
            reasoning
            is a pro not really a con.</p>

        <p>To answer the question about redundancy being good for SSL: redundancy here refers to
            learning the structure of data, as
            Yann mentions. So, for example, take the task "riding a bicycle." This phrase compresses
            an enormous amount of occurrences of riding a bicycle in the world. You can point at so
            many things and correctly call it riding a bicycle. This enables abstract reasoning for
            LLMs, by compression. Right after GPT-3 came out and a lot of people were underestimating
            how far GPTs can go, a few people without academia brain spotted why it's being underestimated:
            people didn't read what eminent AGI researchers (Ilya, Shane Legg, Hutter, ...) had to say about
            text compression:
        </p>
        
        <div align="center">
            <blockquote class="twitter-tweet"><p lang="en" dir="ltr">there are several reasons why it&#39;s the best way:<br><br>1. information dense. text leaves out uncorrelated information about reality in general. images will capture every pixel possible in frame and treat each the same.</p>&mdash; alth0u🤸 (@alth0u) <a href="https://twitter.com/alth0u/status/1295140716771831810?ref_src=twsrc%5Etfw">August 16, 2020</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
        </div>

        <p>
            But compression (and some RLHF to imbue turn-based agency on top) can only take you so far in the physical world.
        </p>
    
        <p>
            When it talks about riding a bicycle, it abstractly does
            understand it. But to actually ride a bicycle, you need to process all the
            context-specific data about riding a bicycle. That means all the sensory data the person
            processes while riding a bicycle, which is going to be in the gigabytes or terabytes if
            you consider all the data (touch, vision, smell, sound, instructions from people,
            observations of others cycling).</p>
        
        <p>
            This goes exactly against the point @alth0u is making in the tweet above about how "text leaves uncorrelated information about reality" - in short it leaves only the non-redundant aspects of reality.
        </p>

        <p>And because you remove all this redundant information (this is what he means by redundant
            - the data I mentioned is redundant to grok the abstract concept of riding a bicycle,
            but not redundant to learn to actually ride one), the model can only answer physics
            intuition puzzles in words, which gives (more often than not, but not always) the illusion that it can now do counterfactual reasoning about real physics (which is what "having a world model" actually means in a technical sense). We also take for granted our ability to have a sense of
            time. In general, we take for granted the vast amount of sensory input we're processing
            to inform the language or code we generate and think language is the hard part (as
            Moravec's paradox says), but it's hard for us <i>because</i> it's at the end of doing vast amounts of SSL.</p>

        <p>The riding a bicycle example is easy because it's in the line of "but can it clean my
            room?" But the same reason why riding a bicycle is hard for an LM is the same reason why
            they score high on Olympiad or graduate textbook questions but score a paltry 4% on SWE
            bench. Because software engineering is not chatting or answering questions in a bounded
            environment. SWE is human labor that requires processing a lot of "redundant" data the
            same way riding a bicycle does, and this data gets lost when you try to compress
            everything into language. LLMs are a great proof for "answering questions about thing
            doesn't mean you understand thing."</p>

        <p>Regarding the lack of sense of time: how would you give it a sense of time, say, for
            running programs? To stop in case one takes too long? Simple: you'd ask to import time
            in the code it writes and print out the time it takes every time the program runs, or
            you augment the inference loop so you're passing in the delta of time from the start of the
            conversation or message, and the model can send a "terminate" signal using a function
            call to stop the program and continue the chat loop to edit the program. But look at
            what you just did! You grounded the language model by passing real information about the
            state of the world, in this case, time. But passing time information is just a few bytes
            because it's only a few tokens ("&lt;number&gt; seconds passed" isn't that many bytes).
            But, as I said before, and as Yann said, to do real-world tasks and ground the language
            model, you need to pass in all the sensory data, which is gonna be a hell of a lot more
            bytes. <em>You're bottlenecked by perception data processing to be used for grounding
                the agent in the real world.</em></p>

        <div align="center"><img src="../images/perception-bottleneck.jpg" width=400></div>

        <p>This is precisely - as Yann explains - what's happening in this Devin demo where they ask
            to control a robot.</p>

        <p>
            And this is also largely what Figure seemed to solve – processing lots of sensory load, and only using the LLM to solve super high level plans. The language modeling is a completely seperate system here and is the orchestrator, but for humans, language generation is more like tool use - it is conditioned on high level plans made in latent representation of the environment, rather than language being the main controller.
        </p>

        <p>
            A tl;dr diagram to go: (I'll add it tomorrow)
        </p>
    </div>
</body>

</html>