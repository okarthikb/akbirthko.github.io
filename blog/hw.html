<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Some Silly Thoughts on Hardware</title>
    <style>
        body {
            font-family: Helvetica, Arial, sans-serif;
            font-size: 14px;
            line-height: 1.6;
            margin: 0 auto;
            max-width: 800px;
            padding: 20px;
        }
        h1 {
            color: #333;
        }
    </style>
</head>
<body>
    <h1>Some Silly Thoughts on Hardware</h1>

    <p><strong>TL;DR: It's not either/or. We'll have transformers and other architectures too.</strong></p>

    <p>It depends on the task at hand. Making an ASIC for transformers doesn't preclude making one for other architectures. The primitives are mostly the same.</p>

    <p>In fact, the Mamba inference step is simpler compared to transformers (see the mamba_step function in my notebook). It's just dot products and elementwise multiplication.</p>

    <p>The real issue with relying solely on transformers requires examining their original purpose. If the goal is to bypass other approaches by generating tokens super fast and spawning numerous agents, letting sheer numbers do the heavy lifting for all tasks, that's likely a losing bet.</p>

    <p>I recommend reading the Mamba paper's section on compression. It reflects deep thinking about hardware and architecture.</p>

    <p>Transformers have many inductive biases that make them less general than often portrayed. Their perceived generality stems from easy-to-understand code and widespread use, but this can be misleading as it's restricted to a particular compute regime.</p>

    <p>You'll hit walls when dealing with extremely long contexts and native multimodal reasoning and planning.</p>

    <p>With text, you need to attend to every token because it's highly compressed. Transformers have a large state size, storing all previous keys and values for each layer.</p>

    <p>This isn't necessary (and is performance-detrimental) when dealing with audio or video. These modalities are more compressible, allowing for real-time compression. You're not bound by context lengths of trillions of tokens, but can intelligently choose what to remember and forget in real time. This is crucial for episodic memory and general computer interfacing.</p>

    <p>Mamba does this partly by having a constant state size, not a growing one, making it real-time by default.</p>

    <p>Most of these models still use similar primitives, namely matrix multiplications. This is unlikely to change significantly.</p>
</body>
</html>
