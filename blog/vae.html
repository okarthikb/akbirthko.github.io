<!DOCTYPE html>
<html>

<head>

  <!-- meta -->
  <meta charset="utf-8" content="width=device-width, initial-scale=1.0" name="viewport" />
  <link href="../style.css" rel="stylesheet" type="text/css" />
  <link href="https://fonts.googleapis.com" rel="preconnect" />
  <link crossorigin="" href="https://fonts.gstatic.com" rel="preconnect" />
  <link href="https://fonts.googleapis.com/css2?family=Fira+Mono&amp;display=swap"
    rel="stylesheet" />

  <!-- mathjax -->
  <script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML"></script>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [
          ['$', '$'],
        ],
        displayMath: [
          ['$$', '$$'],
        ],
        processEscapes: true,
        processEnvironments: true,
        packages: ['base', 'ams']
      },
      svg: {
        fontCache: 'global'
      }
    });
  </script>

  <!-- code highlight -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.7.2/styles/default.min.css"
    rel="stylesheet" />
  <script
    src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.7.2/highlight.min.js"></script>
  <script> hljs.highlightAll(); </script>

  <title>VAEs</title>

</head>

<body>

  <div class="container">
    <h2> Variational Auto-Encoders (personal notes) </h2>

    <p class="border-bottom pad-bottom-25e-2em">
      3 June 2023
      <br />
      <a href="../index.html" id="back">‚Üê</a>
    </p>

    <br />

    <h3> Deriving ELBO for VAE </h3>

    <p>
      The goal of a VAE is to learn a latent representation of the data. We do this by mapping
      the data to a compressed representation, and then reconstructing the data from the compressed
      representation. An encoder (usually a neural net) $q$ takes a datapoint $x$ as input
      and outputs the mean $\mu$ and variance $\sigma$ of a Gaussian distribution from which the
      latent $z$ is sampled. The decoder then reconstructs the data from the latent $z$.
    </p>

    <p>
      VAEs were introduced in this <a href="https://arxiv.org/abs/1312.6114">paper</a> by Kingma et
      al. Their input distribution consists of binary values instead of real values, and the goal
      is to maximize the likelihood of the data for generation. Suppose we sampled $z$ from a
      distribution and then the data $x$ from a distribution conditioned on $z$. The likelihood of
      the data is then
    </p>

    <p class="math">
      $$p(x) = \int p(x\,|\,z)p(z)\,dz$$
    </p>

    <p>
      Maximizing this likelihood using gradient is however intractable, so we instead find a lower
      bound for the likelihood and maximize that. Specifically, we find a lower bound for the log
      likelihood, which is equivalent. Recall from Bayes' theorem that
    </p>

    <p class="math">
      $$p(x\,|\,z) = \frac{p(z\,|\,x)p(x)}{p(z)}$$
    </p>

    <p>
      Now we do a li'l trick. We write the log prob as an expectation over $z$, which lets us do
      some simplification
    </p>

    <p class="math">
      $$\begin{align}
      \log p(x) &= \mathbb{E}_{z \sim q(z\,|\,x)}\left[\log p(x) \right] \\
      &= \mathbb{E}_{z \sim q(z\,|\,x)}\left[\log{
      \frac{p(x\,|\,z)p(z)}{p(z\,|\,x)}
      \frac{q(z\,|\,x)}{q(z\,|\,x)}
      }\right] \\
      &= \mathbb{E}_{z \sim q(z\,|\,x)}\left[\log p(x\,|\,z)\right]
      - \mathbb{E}_{z \sim q(z\,|\,x)}\left[\log{\frac{q(z\,|\,x)}{p(z)}}\right]
      + \mathbb{E}_{z \sim q(z\,|\,x)}\left[\log{
      \frac{q(z\,|\,x)}{p(z\,|\,x)}
      }\right] \\
      &= \mathbb{E}_{z \sim q(z\,|\,x)}\left[\log p(x\,|\,z)\right]
      - D_{KL}\left(q(z\,|\,x) \,||\, p(z)\right)
      + D_{KL}\left(q(z\,|\,x) \,||\, p(z\,|\,x)\right)
      \end{align}$$
    </p>

    <p>
      Note that the last KL divergence term is intractable because it contains $p(z\,|\,x)$, which
      is not easy to compute. But since KL divergence is always greater than or equal to zero, we
      can remove the term and arrive at the lower bound for the log likelihood
    </p>

    <p class="math">
      $$\log p(x) \geq \mathbb{E}_{z \sim q(z\,|\,x)}\left[\log p(x\,|\,z)\right]
      - D_{KL}\left(q(z\,|\,x) \,||\, p(z)\right)$$
    </p>

    <p>
      which is called the Evidence Lower Bound (ELBO) or the variational lower bound.
    </p>

    <br />

    <h3> Closed-form loss expression for Gaussian prior </h3>

    <p>
      In this case,&nbsp;$p(z) = \mathcal{N}(0, I)$, $q$ (the encoder) will be parametrized by
      $\phi$, so it'll be $q_{\phi}(z\,|\,x)$, and the decoder will be parametrized by $\theta$, so
      it'll be $p_{\theta}(x\,|\,z)$. Let's consider the KL divergence term first
    </p>

    <p class="math">
      $$\begin{align*}
      D_{KL}(q_{\phi}(z\,|\,x)\,\vert\vert\,p(z))
      &= \int q_{\phi}(z\,|\,x)\log{\frac{q_{\phi}(z\,|\,x)}{p(z)}}\,dz
      \end{align*}$$
    </p>

    <p>
      Our probability distributions are Gaussians. Given $x$, the encoder will predict the mean
      $\mu$ and variance $\sigma^2$, so
    </p>

    <p class="math">
      $$\begin{align*}
      q_{\phi}(z\,|\,x) &= \mathcal{N}(z\,|\,\mu, \sigma^2) \\
      &= \frac{1}{\sqrt{2\pi\sigma^2}}\exp{\left(-\frac{(z - \mu)^2}{2\sigma^2}\right)}
      \end{align*}$$
    </p>

    <p>
      The prior $p(z)$ is just the standard Gaussian $\mathcal{N}(0, 1)$ so our KL divergence after
      substituting becomes
    </p>

    <p class="math">
      $$D_{KL}(q_{\phi}(z\,|\,x)\,\vert\vert\,p(z)) = \int
      \frac{1}{\sqrt{2\pi\sigma^2}}\exp{\left(-\frac{(z - \mu)^2}{2\sigma^2}\right)}
      \log{\frac{\frac{1}{\sqrt{2\pi\sigma^2}}\exp{\left(-\frac{(z - \mu)^2}{2}\right)}}
      {\frac{1}{\sqrt{2\pi}}\exp(-z^2)}}\,dz$$
    </p>

    <p>
      Skipping the rather tedious
      <a href="https://arxiv.org/pdf/1907.08956.pdf">derivation</a> here, but the above simplifies
      to
    </p>

    <p class="math">
      $$D_{KL}(q_{\phi}(z\,|\,x)\,\vert\vert\,p(z)) =
      -\frac{1}{2}\sum_{j=1}^J\left(1 + \log{\sigma_j^2} - \mu_j^2 - \sigma_j^2\right)$$
    </p>

    <p>
      where $j$ is the mean and variance vector index and $J$ is the dimension of the latent.
      Now we consider the expected log probability term. The probability is parametrized by $\theta$
      since it depends on the mean and standard deviation predicted by the decoder
    </p>

    <p class="math">
      $$\begin{align*}\log p_{\theta}(x\,|\,z)
      &= \log{\frac{1}{\sqrt{2\pi \sigma^2}}\exp{\left(-\frac{(x - \mu)^2}{2\sigma^2}\right)}} \\
      &= -\frac{1}{2}\log{2\pi\sigma^2} - \frac{(x - \mu)^2}{2\sigma^2} \\
      &= -\log{\sigma} - \frac{(x - \mu)^2}{2\sigma^2}
      \end{align*}$$
    </p>

    <p>
      where we ignored the constant $\log{2\pi}$. We want the expected log prob though, so we just
      sample a few $x$s (say, L of them) and take the mean
    </p>

    <p class="math">
      $$\mathbb{E}_{z \sim q(z\,|\,x)}\left[\log p_{\theta}(x\,|\,z)\right]
      = \frac{1}{L}\sum_{l=1}^L\left(-\log{\sigma} - \frac{(x_l - \mu)^2}{2\sigma^2}\right)$$
    </p>

    <p>
      So the objective we're maximizing is
    </p>

    <p class="math">
      $$\begin{align*}
      \mathcal{G}(\theta, \phi)
      &= \mathbb{E}_{z \sim q(z\,|\,x)}\left[\log p_{\theta}(x\,|\,z)\right]
      + D_{KL}(q_{\phi}(z\,|\,x)\,\vert\vert\,p(z)) \\
      &= \frac{1}{L}\sum_{l=1}^L
      \sum_{k=1}^K\left(-\log{\sigma_k} - \frac{(x_{l,k} - \mu_k)^2}{2\sigma_k^2}\right)
      + \frac{1}{2}\sum_{j=1}^J\left(1 + \log{\sigma_j^2} - \mu_j^2 - \sigma_j^2\right)
      \end{align*}$$
    </p>

    <p>
      where $K$ is dimension of the output. The loss is just the negative of the above expression
      (so we minimize it)
    </p>

    <p class="math">
      $$\mathcal{L}(\theta, \phi) = \frac{1}{L}\sum_{l=1}^L
      \sum_{k=1}^K\left(\log{\sigma_k} + \frac{(x_{l,k} - \mu_k)^2}{2\sigma_k^2}\right)
      - \frac{1}{2}\sum_{j=1}^J\left(1 + \log{\sigma_j^2} - \mu_j^2 - \sigma_j^2\right)$$
    </p>

    <p>
      and this is the loss for a single data point. For a batch, we just take the sum of the losses
      of each datapoint. As we can see, the expected log prob or the reconstruction loss
      looks like mean squared error for a Gaussian distribution. We can use other losses as
      well. If the data is say, normalized images, we can use binary cross-entropy loss instead.
    </p>

    <br />

    <h3> Implementation </h3>

    <p>
      Colab notebook
      <a href="https://colab.research.google.com/drive/1WJWfc6KZ_SY9fgsChWi_JUYHEMAuH-NN?usp=sharing">here</a>.
      We'll use MNIST. A simple MLP for the encoder and decoder (up convolution) suffices. A
      slight modification we do is that the encoder outputs the log variance instead of the
      variance itself. This is because we want the variance to be always positive and
      exponentiating the log variance will do that. Let's define the VAE class:
    </p>

    <pre class="language-python"><code>class VAE(nn.Module):
  def __init__(self, zdim):
    super().__init__()
    self.zdim = zdim

    self.encoder = nn.Sequential(
      nn.Linear(784, 256),
      nn.ELU(),
      nn.Linear(256, 64),
      nn.ELU(),
      nn.Linear(64, 32),
      nn.ELU(),
      nn.Linear(32, 2 * zdim)
    )

    self.decoder = nn.Sequential(
      nn.Linear(zdim, 32),
      nn.Linear(32, 64),
      nn.ELU(),
      nn.Linear(64, 256),
      nn.ELU(),
      nn.Linear(256, 784),
      nn.Sigmoid()
    )</code></pre>

    <p>
      This has around 440k parameters. We've used <span class="inline-code">ELU</span> because
      <span class="inline-code">ReLU</span> can cause dead neurons. Since we're using
      <span class="inline-code">Sigmoid</span> in the output layer, we'll use binary cross-entropy
      loss as the reconstruction loss instead of the MSE loss we derived above. We define the
      forward pass, where we use the repametrization trick.
    </p>

    <pre class="language-python"><code>def forward(self, x):
  mu, logvar = torch.split(self.encoder(x), self.zdim, dim=1)
  std = (logvar * 0.5).exp()

  # reparametrization trick
  z = std * torch.randn(mu.shape).to(mu.device) + mu

  return self.decoder(z), mu, std, logvar</code></pre>

    <p>
      Why do we use the reparametrization trick? So our network is differentiable. Remember that
      sampling is not a differentiable operation, so we make use of the fact that
      $c\sigma(x) = \sigma(cx)$ and $c\mathbb{E}[x] = \mathbb{E}[cx]$ to make the sampling
      operation differentiable. Now we define the loss function.
    </p>

    <pre><code>def loss(self, x):
  rx, mu, std, logvar = self(x)
  gll = F.binary_cross_entropy(rx, x, reduction='sum')
  kld = -0.5 * (1 + logvar - mu * mu - std * std).sum()
  return gll + kld</code></pre>

    <p>
      As we can see, PyTorch notation is much more concise than the math notation. The training
      loop is pretty straightforward. We first flatten every MNIST image because we're using
      an MLP.
    </p>

    <pre class="language-python"><code>device = torch.device('cuda')
batch_size = 128
epochs = 50
lr = 3e-4
zdim = 2

transform = transforms.Compose([
  transforms.ToTensor(),
  transforms.Lambda(lambda x: torch.flatten(x))
])

def train(model, batch_size, lr, epochs):
  loader = torch.utils.data.DataLoader(
    MNIST('.', download=True, transform=transform),
    batch_size=batch_size,
    shuffle=True
  )

  opt = torch.optim.Adam(model.parameters(), lr=lr)

  for epoch in range(1, epochs + 1):
    bar = tqdm(loader, ascii=' >=')
    for x, _ in bar:
      opt.zero_grad()
      loss = model.loss(x.to(device))
      loss.backward()
      opt.step()
      bar.set_postfix({'loss': f'{loss.item():.6f}'})</code></pre>

    <p>
      We chose a <span class="inline-code">zdim</span> of 2 so that we can visualize the
      latent space easily because every decoder output corresponds to a point in $\mathbb{R}^2$.
      Visualizing decoder output for latent vectors in $[-2, 2]^2$ gives us this:
    </p>

    <div class="image-container">
      <div class="image">
        <img src="../images/vae_mnist.png" alt="VAE latent space"
          style="width: 768px; height: auto;">
      </div>
    </div>

  </div>

</body>

</html>
